import time
import os
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
import streamlit as st
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler

st.set_page_config(
    page_title="Document GPT",
    page_icon="ğŸ“„",
    layout="wide",
)


class ChatCallbackHandler(BaseCallbackHandler):

    def __init__(self):
        self.message = ""
        self.message_box = None

    def on_llm_start(self, *args, **kwargs):
        self.message = ""
        self.message_box = st.empty()
        
    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token: str, **kwargs):
        self.message += token
        if self.message_box is not None:
            self.message_box.markdown(self.message)

llm = ChatOpenAI(model="gpt-4.1-nano", temperature=0.1, streaming=True, callbacks=[ChatCallbackHandler()])


@st.cache_data(show_spinner="Embedding file..." )
def embed_file(file):
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not os.getenv("OPENAI_API_KEY"):
            st.error("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            st.info("í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ .env íŒŒì¼ì— API í‚¤ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
            return None
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("./.cache/files", exist_ok=True)
        os.makedirs("./.cache/embeddings", exist_ok=True)
        
        file_content = file.read()
        file_path = f"./.cache/files/{file.name}"
        
        with open(file_path, "wb") as f:
            f.write(file_content)
        
        cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
        
        splitter = CharacterTextSplitter.from_tiktoken_encoder(
            separator="\n",
            chunk_size=600,
            chunk_overlap=100,
        )
        
        with st.spinner("ğŸ“„ ë¬¸ì„œë¥¼ ë¡œë”©í•˜ê³  ë¶„í• í•˜ëŠ” ì¤‘..."):
            loader = UnstructuredFileLoader(file_path)
            docs = loader.load_and_split(text_splitter=splitter)
            st.success(f"âœ… {len(docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ!")
        
        with st.spinner("ğŸ”¤ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            embeddings = OpenAIEmbeddings()
            cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
            vectorstore = FAISS.from_documents(docs, cached_embeddings)
            retriever = vectorstore.as_retriever()
            st.success("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        
        return retriever
        
    except Exception as e:
        st.error(f"âŒ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        st.info("ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
        st.info("1. OPENAI_API_KEYê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€")
        st.info("2. ì¸í„°ë„· ì—°ê²°ì´ ì•ˆì •ì ì¸ì§€")
        st.info("3. ì—…ë¡œë“œí•œ íŒŒì¼ì´ ì†ìƒë˜ì§€ ì•Šì•˜ëŠ”ì§€")
        return None


st.title("DocumentGPT")

st.markdown(
    """
Welcome!
            
Use this chatbot to ask questions to an AI about your files!

Upload a file on the sidebar to get started!
"""
)

with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],
)

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def save_message(message, role):
    st.session_state["messages"].append({"role": role, "message": message})

def paint_history():
    for message in st.session_state["messages"]:
        send_message(message["message"], message["role"], save=False)

prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a helpful assistant that can answer questions about the documents you are given. If you don't know the answer, just say that you don't know. Do not make up an answer. Always answer in Korean.
    Context: {context}""",),

    ("user", "{question}"),
])

if file:
    retriever = embed_file(file)
    
    if retriever:
        st.success("ğŸ‰ íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        send_message("I'm ready to answer your questions!", "ai", save=False)
        paint_history()
        message = st.chat_input("Ask me anything!")
        if message:
            send_message(message, "human")
            chain = {
                "context": retriever|RunnableLambda(lambda x: "\n\n".join([doc.page_content for doc in x])),
                "question": RunnablePassthrough()
            } | prompt | llm 
            with st.chat_message("ai"):
                response = chain.invoke( message)
                
           
    else:
        st.warning("íŒŒì¼ ì²˜ë¦¬ë¥¼ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìœ„ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

else:
    st.session_state["messages"] = []
